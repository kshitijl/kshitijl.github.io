---
layout: post
title:  "How big is my L1 cache?"
date:   2016-02-24 00:05:26 -0500
categories: measurement
---

### Modern CPUs are faster than older ones

That's not just because they pack more silicon, but also because their
architecture is more clever. The most fundamental of these
architectural ideas is the memory hierarchy, or _caching_. When a
program asks for some data in RAM, it is first put into a smaller,
faster block of memory that's closer to the CPU. The hope is that the
program will ask for that same bit of data again, and when that
happens it can be served much more quickly.

Like many other microarchitectural improvements (like prefetching,
pipelining, branch prediction or out-of-order execution), caching
offers a transparent performance boost to all programs. This means
that old programs will run faster on the new, smarter hardware. It
also means that programs written in high-level languages will benefit
even though those languages typically make no affordances to such
low-level concerns.

But, as we will see in a future post about matrix multiplication, __a
program designed with the cache hierarchy in mind can be orders of
magnitude faster than a naive implementation__.

This is a fact that many programmers have heard but few have seen
firsthand. I think that makes it hard to know what to do with it. In a
complex programming environment with a high-level language and its
runtime, it can be hard to imagine what, exactly, it would mean to
take advantage of the memory hierarchy.

### Let's detect the L1 cache

As a first step towards becoming cache-aware, let's write a simple
program whose sole purpose is to exhibit the existence of a cache. The
L1 cache is the first in the hierarchy so we will try to detect it. In
particular, we will try to figure out its size.

How can we detect the size of the smallest cache? Well, a program that
only works with a tiny amount of data will never access RAM after the
first time, and so it'll run very fast. As the working set grows
bigger, at some point it will no longer fit in the smallest cache and
we will see a substantial slowdown.

So: the we're looking for a _discontinuity_ in the graph of
performance vs working-set size. 

What work should we do on our data? To avoid complicating our
measurement, let's do the simplest thing possible that still uses data:
doubling all the elements in the data set.

In pseudocode:

{% highlight python %}
for working_set_size in [1024,
    working_set = allocate(working_set_size)
    for ii in range(working_set_size):
        working_set[ii] *= 2
{% endhighlight %}
### That overestimates the L1 cache size

Unfortunately our measurement will be frustrated by another
microarchitectural innovation: _prefetching_. Because the patten in
which we access the data (linear streaming) is so common and easy to
predict, modern hardware simply fetches into cache before we even ask
for it. Prefetching has the effect of making caches look larger than
they really are, and to spread out the discontinuity in the graph of
performance vs size.

Another complication is automatic compiler _vectorization_: modern
processors offer instructions that act on several scalar values at
once. Compilers can recognize common patterns where vectorization is
safe and automatically optimize your code to use these instructions.
Doubling an array is a classic example where vectorization is safe and
easy: the array is already laid out sequentially in memory and there
are instructions to add two vectors together. This complication is not
as serious as prefetching because it does not influence caching
behavior as directly. But we have full control so we will be
scientific and try to isolate the phenomenon we're studying.

A final complication is that modern processors can execute several
instructions at once if they are independent. This is called
superscalar execution and it happens transparently and automatically
at the hardware level. In the benchmark we wrote, there is the
potential for superscalar execution because every multiplication is
independent of every other. If the the compiler unrolls the loop, the
hardware could have an easy time executing several multiplications at
once.

### Isolating the cache

There's an easy way to eliminate all these sources of error: access
the data in a random way such that the location of the next access
depends on the result of the previous one. That way, the processor
can't know what to prefetch and it can't execute several instructions
in parallel.

We will use the fact that floating-point addition is not associative
(read about Kahan summation if this is new to you). The processor is
forced to execute additions in order so that the result is correct.
Instead of doubling every element in an array, we will add together a
bunch of entries in the array (perhaps reusing some of them). Which
values we add together, and the order in which we add them, will be
randomized based on the running sum so far. One nice consequence of
this is that we won't have to scale the time taken by the size of the
working set: we always perform the same number of additions.

An easy way to do this is to take the current value of the sum,
interpret its bits as 64-bit address, and mod by the size of the
working set. This is probably sufficiently random to fool the
prefetcher. If you're worried it isn't, you can xor the running sum
with the current data value before generating the next address.

In pseudocode:

{% highlight python %}
M = 1e6
for working_set_size in [1024, 2048, ...]:
	working_set = allocate(working_set_size)
	result = 0
	for ii in range(M):
		address = (unsigned 64 bit number)result % working_set_size
		result += working_set[address]
{% endhighlight %}

